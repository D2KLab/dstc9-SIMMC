{
  "split": support.extract_split_from_filename(json_path),
  "version": 1.0,
  "year": 2020,
  "domain": FLAGS.domain,
  "dialogue_data": [
  {
    “dialogue”: [
      {
        “belief_state”: [
          {
            “act”: <str>,
            “slots”: [
              [ <str> slot_name, <str> slot_value  ], // end of a slot name-value pair
              ...
            ]
          }, // end of an act-slot pair
          ...
        ],
        “domain”: <str>,
        “raw_assistant_keystrokes”: <dict>,
        “state_graph_{idx}”: <dict>,
        “system_belief_state”: <dict>,
        “system_transcript”: <str>,
        “system_transcript_annotated”: <str>,
        “transcript”: <str>,
        “transcript_annotated”: <str>,
        “turn_idx”: <int>,
        “turn_label”: [ <dict> ],
        “visual_objects”: <dict>
      }, // end of a turn (always sorted by turn_idx)
      ...
    ],
    “dialogue_coref_map”: {
      // map from object_id to local_id re-indexed for each dialog
      <str>: <int>
    },
    “dialogue_idx”: <int>,
    “domains”: [ <str> ]
  }
]
}

----------------------------------------------------------------------------------------------

- belief state: contains the action and the slots with the mapping slot->value. 
                The length can be > 1 if multiple possible actions are possible in some turn.
- domain: fashion or furniture
- raw_assistant_keystrokes: the raw UI interactions made by the human Assistant (wizard) using the Unity interface during data collection. 
                            [[ These are very noisy as the human assistants’ tended to explore the catalogue before settling on and sharing their view. 
                            The Unity UI also operated asynchronously to the messaging interface. ]] 
                            We distil target actions for the action prediction task (sub-task #1) from these raw keystrokes and NLU/NLG annotation.
- state_graph_{idx}: refers to the graph representation of the cumulative dialog and the multimodal contexts known to the user, 
                    each at a different phase during the dialog (e.g. via a multimodal action of showing items, 
                    an assistant providing information, a user providing preferences, etc.).
                    - state_graph_0: initial state before the start of the user utterance
                    - state_graph_1: state modified after the user utterance 
                    - state_graph_2: final state modified after the assistant utterance & assistant action.